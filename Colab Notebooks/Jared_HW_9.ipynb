{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Jared_HW_9.ipynb","provenance":[{"file_id":"1dvyZbxX-PyyA7ova3YKmTwHzoNOb-CEX","timestamp":1619401343390},{"file_id":"1q132CDBOx30D1Oo0-N2ZTiRi7oULMzGD","timestamp":1613769499165},{"file_id":"15HCslw2TW3GIPK3yTHoayK3y5Du0ifTF","timestamp":1611954396426}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"cB-ZphMl-MOW"},"source":["# SPRING 2021\n","# Applied Machine Learning, HW 9 Solutions\n","\n","<b><font color='red'>Instructions:</font></b>  Read all problems very carefully. Make sure that you are answering all questions included in a given problem. For every question, you are provided with a code cell to show your solution; be sure to run the cell by pressing the SHIFT and ENTER buttons simultaneously. The output of the cell must demonstrate the correctness of your solution. If you need to insert more code cells -- feel free to do so."]},{"cell_type":"markdown","metadata":{"id":"tt2vdGTnKKqt"},"source":["**Problem 1** \n","\n","<img src='https://drive.google.com/uc?export=view&id=1v9dyG8EBMou_M_VlOj8-Rrqy9IG9686G' width='300'>\n","\n","1. In this problem, you will continue working with the famous \"Titanic\" data set. Last week, we built a logistic regression model to predict the survival status of Titanic's passengers. This week, we will do the same using the K-Nearest Neighbors (KNN) method; we will also apply grid search to find the best number of nearest neigbors for the KNN classifier.\n","\n","  First, run the code cell below to load the data set.  "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CQZPFwj7XtzR","executionInfo":{"status":"ok","timestamp":1619401533972,"user_tz":300,"elapsed":2236,"user":{"displayName":"Jared Ren","photoUrl":"","userId":"05261194671062343283"}},"outputId":"9f8f3620-90ea-4490-deaf-79bf11bd20a9"},"source":["import pandas as pd\n","from seaborn import load_dataset\n","\n","df=load_dataset('titanic')\n","cols=[0, 1, 2, 3, 6]\n","df=df.iloc[:, cols]\n","# 0 - male; 1 - female\n","df['sex']=(df['sex']=='female').astype(int).values\n","\n","df.dropna(inplace=True)\n","\n","df.info()"],"execution_count":1,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 714 entries, 0 to 890\n","Data columns (total 5 columns):\n"," #   Column    Non-Null Count  Dtype  \n","---  ------    --------------  -----  \n"," 0   survived  714 non-null    int64  \n"," 1   pclass    714 non-null    int64  \n"," 2   sex       714 non-null    int64  \n"," 3   age       714 non-null    float64\n"," 4   fare      714 non-null    float64\n","dtypes: float64(2), int64(3)\n","memory usage: 33.5 KB\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"i0qBQ7XSq_SU"},"source":["2. Review the \"Titanic\" problem from the last week's homework assigment to refresh your memory of this data set. Recall that the data frame `df` has the following columns:\n","\n","  * `survived`: the passenger's survival status (0 - drowned, 1 - survived);\n","  * `pclass`: the passenger's ticket class (1 - 1st, 2 - 2nd, 3 - 3rd);\n","  * `sex`: the passenger's gender (0 - male, 1 - female);\n","  * `age`: the passenger's age in years;\n","  * `fare`: the passenger's ticket fare.\n","\n","  In this week's version of the data frame, all missing values has already been removed as can be seen from the output of the `info()` method above."]},{"cell_type":"markdown","metadata":{"id":"vkJ3yrrQxX8o"},"source":["3. Split the data into the training and testing sets: place 20% of the data into the testing set `X_test`, `y_test` and the rest into the training set `X_train`, `y_train`. Make sure that the proportion of the survived passengers (positive class) is the same in both training and testing sets by utilizing the `stratify` option. Use a random state of 42 when running the `train_test_split()` function."]},{"cell_type":"code","metadata":{"id":"g5UcYwAPgztr","executionInfo":{"status":"ok","timestamp":1619402219768,"user_tz":300,"elapsed":1015,"user":{"displayName":"Jared Ren","photoUrl":"","userId":"05261194671062343283"}}},"source":["from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42, stratify=y )"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3apc1sU-TChQ"},"source":["4. Build a two-step pipeline `pipe_knn` including the following steps:\n","  * Normalizing the data using the standard scaler approach;\n","  * K-Nearest Neighbors classifier (at this point, do not specify the number of nearest neighbors)."]},{"cell_type":"code","metadata":{"id":"ot-lFTGiSxbN","executionInfo":{"status":"ok","timestamp":1619402331154,"user_tz":300,"elapsed":255,"user":{"displayName":"Jared Ren","photoUrl":"","userId":"05261194671062343283"}}},"source":["from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.neighbors import KNeighborsClassifier\n","\n","pipe_knn = Pipeline([\n","                     ('sc', StandardScaler()),\n","                     ('knn', KNeighborsClassifier())\n","])"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X9jz_FhD39RK"},"source":["5. Instantiate an object of the `GridSearchCV()` class called `gs` and perform a 5-fold cross-validated grid search on your training data to determine the optimal number of nearest neighbors $K$ for the pipeline introduced in the previous step. Explore the values of $K$ in the range from 1 to 10 and set `scoring='accuracy'` (you might want to review our \"Validation and Fine-Tuning\" lecture notes and/or homework 7 to reshresh your memory of grid search)."]},{"cell_type":"code","metadata":{"id":"8F4iYWsyRY6I","executionInfo":{"status":"ok","timestamp":1619401533974,"user_tz":300,"elapsed":2230,"user":{"displayName":"Jared Ren","photoUrl":"","userId":"05261194671062343283"}}},"source":["from sklearn.model_selection import GridSearchCV\n","\n","gs = GridSearchCV(pipe_knn, param_grid=, )"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7A8T-2vsViqg"},"source":["6. Print to the screen the best value of $K$ found in the grid search."]},{"cell_type":"code","metadata":{"id":"Blq-gpKBVhpD","executionInfo":{"status":"ok","timestamp":1619401533975,"user_tz":300,"elapsed":2230,"user":{"displayName":"Jared Ren","photoUrl":"","userId":"05261194671062343283"}}},"source":[""],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ojKkHNt-4Ry0"},"source":["7. Use `gs` to compute predictions for the survival status on the test set data. Store this result in a variable called `y_pred`."]},{"cell_type":"code","metadata":{"id":"cr2wQrLQgk2W","executionInfo":{"status":"ok","timestamp":1619401533975,"user_tz":300,"elapsed":2227,"user":{"displayName":"Jared Ren","photoUrl":"","userId":"05261194671062343283"}}},"source":[""],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ACQT6VRaXR_A"},"source":["8. Compute the accuracy of your predictions."]},{"cell_type":"code","metadata":{"id":"kKJmP9HTXXdZ","executionInfo":{"status":"ok","timestamp":1619401533976,"user_tz":300,"elapsed":2227,"user":{"displayName":"Jared Ren","photoUrl":"","userId":"05261194671062343283"}}},"source":[""],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kIj5prn3EP90"},"source":["9. Compute the confusion matrix and print it to the screen in the form of an array and in the graphical form. Set `values_format = 'd'` when plotting the confusion matrix to supress the scientific notation format and rounding. "]},{"cell_type":"code","metadata":{"id":"ELEY8TrJ6DdU","executionInfo":{"status":"ok","timestamp":1619401533976,"user_tz":300,"elapsed":2225,"user":{"displayName":"Jared Ren","photoUrl":"","userId":"05261194671062343283"}}},"source":[""],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"4wCZ9gsLE_qQ","executionInfo":{"status":"ok","timestamp":1619401533977,"user_tz":300,"elapsed":2225,"user":{"displayName":"Jared Ren","photoUrl":"","userId":"05261194671062343283"}}},"source":[""],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W-RK0WlAZNBe"},"source":["10. What are the numbers of true positives (TP), true negatives (TN), false negatives (FN), and false positives (FP)? Note that the positive class here includes those passengers who survived the disaster."]},{"cell_type":"markdown","metadata":{"id":"aTrw1PaQFAP8"},"source":["**Answers:**\n","\n","* TP = \n","* TN = \n","* FN = \n","* FP = "]},{"cell_type":"markdown","metadata":{"id":"0qQIwZNqGFvn"},"source":["**Problem 2**\n","\n","1. In this problem, you will continue working with the MNIST data set that was introduced last week. This time we will buld a multiclass classifier for these data; we will try both Softmax regression and KNN.\n","\n","  Start by refreshing your memory of the data set: review Problem 2 of homework 8. Then run the code cell below to fetch the MNIST data. The data set is quite large, so it might take a minute or two to load it, so please be patient."]},{"cell_type":"code","metadata":{"id":"A9X_UH8FFX4l","executionInfo":{"status":"ok","timestamp":1619401566523,"user_tz":300,"elapsed":34769,"user":{"displayName":"Jared Ren","photoUrl":"","userId":"05261194671062343283"}}},"source":["import numpy as np\n","from sklearn.datasets import fetch_openml\n","mnist = fetch_openml('mnist_784', version=1)\n","X, y = mnist['data'], mnist['target'].astype(np.uint8)"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ol-lgSS0SWtP"},"source":["2. The features and labels are now stored in NumPy arrays `X` and `y`, respectively. Split the data set into the training (80%) and testing set (20%) stratifying on the target variable `y`. Use a random state of 42 when running the `train_test_split()` function."]},{"cell_type":"code","metadata":{"id":"gLABxTNwfiLz","executionInfo":{"status":"ok","timestamp":1619401566526,"user_tz":300,"elapsed":34770,"user":{"displayName":"Jared Ren","photoUrl":"","userId":"05261194671062343283"}}},"source":[""],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fIhD0AZBhDC-"},"source":["6. Create a new 2-step pipeline `smax` containing a standard scaler and a softmax classifier. Set the `tol` parameter of the `LogisticRegression()` class to 100 to avoid a convergence warning (this parameter controls tolerance for stopping criteria). Fit the pipeline on the training data; then make predictions on the test data and compute the accuracy of these predictions. Print the accuracy value to the screen."]},{"cell_type":"code","metadata":{"id":"kbcfWah9O2CW","executionInfo":{"status":"ok","timestamp":1619401566526,"user_tz":300,"elapsed":34769,"user":{"displayName":"Jared Ren","photoUrl":"","userId":"05261194671062343283"}}},"source":[""],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fQBjfEmp5e1W"},"source":["7. Create a classification report for the Softmax classifier and print it to the screen (use either the KNN lecture notes or your practicum notebook # 4 to see how we did it in class)."]},{"cell_type":"code","metadata":{"id":"boNjzSKJ5jki","executionInfo":{"status":"ok","timestamp":1619401566526,"user_tz":300,"elapsed":34767,"user":{"displayName":"Jared Ren","photoUrl":"","userId":"05261194671062343283"}}},"source":[""],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vvxnOqHByAzT"},"source":["8. Create a new 2-step pipeline knn containing a standard scaler and a KNN classifier (set the number of nearest neighbors to 5). Fit the pipeline on the training data; then make predictions on the test data and compute the accuracy of these predictions. Print the accuracy value to the screen.\n","\n","  WARNING: This step typically takes 20-30 minutes of compute time, so please be patient -- just leave the notebook running, go and do something fun, and come back when the process is finished."]},{"cell_type":"code","metadata":{"id":"vveJnZQFzKyk","executionInfo":{"status":"ok","timestamp":1619401566527,"user_tz":300,"elapsed":34766,"user":{"displayName":"Jared Ren","photoUrl":"","userId":"05261194671062343283"}}},"source":[""],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Evlonmqq462V"},"source":["9. Create a classification report for the KNN classifier and print it to the screen."]},{"cell_type":"code","metadata":{"id":"P4c2zA1T5Nu6","executionInfo":{"status":"ok","timestamp":1619401566527,"user_tz":300,"elapsed":34765,"user":{"displayName":"Jared Ren","photoUrl":"","userId":"05261194671062343283"}}},"source":[""],"execution_count":2,"outputs":[]}]}